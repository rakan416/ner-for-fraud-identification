{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8582fff8",
   "metadata": {},
   "source": [
    "Dataset yang digunakan untuk fine tuning dan transfer learning model IndoBERT sebanyak 1057 teks dengan proporsi data train test 9:1. Pelabelan data dilakukan dengan manual dan dibantu oleh generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14edb2",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "da1a510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96441934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoProcessor, TrainingArguments, Trainer, pipeline, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953e1f4",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c67655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37979 entries, 0 to 37978\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tokens  36923 non-null  object\n",
      " 1   labels  36923 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 593.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/NER_DATASET.tsv', sep='\\t')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51e6216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "O             30376\n",
       "B-ACTION       1758\n",
       "B-MODUS        1718\n",
       "               1056\n",
       "B-PRODUK        705\n",
       "B-PERSON        469\n",
       "B-PLATFORM      349\n",
       "B-NOMINAL       299\n",
       "I-PRODUK        219\n",
       "B-LAYANAN       218\n",
       "I-MODUS         218\n",
       "I-PERSON        113\n",
       "B-KONTAK        104\n",
       "B-REK           101\n",
       "I-LAYANAN        78\n",
       "I-NOMINAL        70\n",
       "I-KONTAK         36\n",
       "I-REK            34\n",
       "I-PLATFORM       30\n",
       "I-ACTION         28\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(\"\", inplace=True)\n",
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6209fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sebetulnya', 'bukannya', 'nggak', 'mau', 'usaha', 'tapi', 'banyak', 'penipuan', 'mindset', 'udah', 'bener', 'tapi', 'kalo', 'misalnya', 'tools', 'nya', 'ternyata', 'menuju', 'scam', 'gimana', '?', 'ketipu', 'terus', 'donk'], ['sebelumnya', 'gw', 'juga', 'pernah', 'hampir', 'ketipu', 'via', 'interview', 'palsu', 'di', 'linkedin', '.', 'pernah', 'gw', 'tulis', 'di', 'blog', 'ini', ':', 'dan', 'sekarang', '.', 'kejadian', 'mirip', 'keulang', 'lagi', 'bedanya', 'kali', 'ini', 'gw', 'udah', 'diblok', 'duluan', '.', 'chat', 'penawarannya', 'ialng', 'jir', 'keburu', 'diblok', 'duluan', '2/9']]\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MODUS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MODUS', 'O', 'O', 'B-ACTION', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-ACTION', 'O', 'B-MODUS', 'I-MODUS', 'O', 'B-PLATFORM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ACTION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ACTION', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "all_sentences = []\n",
    "all_labels = []\n",
    "\n",
    "tokens = []\n",
    "ner_tags = []\n",
    "for idx, row in df.iterrows():\n",
    "    token = row['tokens']\n",
    "    label = row['labels']\n",
    "\n",
    "    if token == \"\" and label == \"\":\n",
    "        if tokens:\n",
    "            all_sentences.append(tokens.copy())\n",
    "            all_labels.append(ner_tags.copy())\n",
    "            tokens.clear()\n",
    "            ner_tags.clear()\n",
    "    else:\n",
    "        tokens.append(token)\n",
    "        ner_tags.append(label)\n",
    "\n",
    "if tokens:\n",
    "    all_sentences.append(tokens.copy())\n",
    "    all_labels.append(ner_tags.copy())\n",
    "    tokens.clear()\n",
    "    ner_tags.clear()\n",
    "\n",
    "print(all_sentences[:2])\n",
    "print(all_labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e612a4",
   "metadata": {},
   "source": [
    "Membuat struktur data sequence:\n",
    "\n",
    "``` python\n",
    "all_sentences = [[sentence], [sentence], ... ]\n",
    "all_labels = [[ner_tags], [ner_tags], ... ]\n",
    "```\n",
    "\n",
    "Setiap sequence dibungkus dengan list, begitu juga dengan tag NER-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aec924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ACTION', 'B-KONTAK', 'B-LAYANAN', 'B-MODUS', 'B-NOMINAL', 'B-PERSON', 'B-PLATFORM', 'B-PRODUK', 'B-REK', 'I-ACTION', 'I-KONTAK', 'I-LAYANAN', 'I-MODUS', 'I-NOMINAL', 'I-PERSON', 'I-PLATFORM', 'I-PRODUK', 'I-REK', 'O']\n",
      "{'B-ACTION': 0, 'B-KONTAK': 1, 'B-LAYANAN': 2, 'B-MODUS': 3, 'B-NOMINAL': 4, 'B-PERSON': 5, 'B-PLATFORM': 6, 'B-PRODUK': 7, 'B-REK': 8, 'I-ACTION': 9, 'I-KONTAK': 10, 'I-LAYANAN': 11, 'I-MODUS': 12, 'I-NOMINAL': 13, 'I-PERSON': 14, 'I-PLATFORM': 15, 'I-PRODUK': 16, 'I-REK': 17, 'O': 18}\n",
      "{0: 'B-ACTION', 1: 'B-KONTAK', 2: 'B-LAYANAN', 3: 'B-MODUS', 4: 'B-NOMINAL', 5: 'B-PERSON', 6: 'B-PLATFORM', 7: 'B-PRODUK', 8: 'B-REK', 9: 'I-ACTION', 10: 'I-KONTAK', 11: 'I-LAYANAN', 12: 'I-MODUS', 13: 'I-NOMINAL', 14: 'I-PERSON', 15: 'I-PLATFORM', 16: 'I-PRODUK', 17: 'I-REK', 18: 'O'}\n"
     ]
    }
   ],
   "source": [
    "LABEL = sorted(set(lbl for tags in all_labels for lbl in tags))\n",
    "LABEL2ID = {lbl : i for i, lbl in enumerate(LABEL)}\n",
    "ID2LABEL = {i : lbl for lbl, i in LABEL2ID.items()}\n",
    "\n",
    "print(LABEL)\n",
    "print(LABEL2ID)\n",
    "print(ID2LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e6caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labelids = [[LABEL2ID[lbl] for lbl in tags] for tags in all_labels]\n",
    "dataset = [{\"sentence\": sentence, \"ner_tags\": tags} for sentence, tags in zip(all_sentences, all_labelids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2912866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data untuk train : 950\n",
      "Total data untuk validation : 106\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "print(f\"Total data untuk train : {len(train_data)}\")\n",
    "print(f\"Total data untuk validation : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03090507",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = DatasetDict({\n",
    "    \"train\" : Dataset.from_list(train_data),\n",
    "    \"validation\" : Dataset.from_list(val_data)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb1a01",
   "metadata": {},
   "source": [
    "## Model Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5482fc",
   "metadata": {},
   "source": [
    "### Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e15918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"indobenchmark/indobert-base-p2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(LABEL),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498176f",
   "metadata": {},
   "source": [
    "### Tokenizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c785fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=250,\n",
    "        stride=128\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    sample_mapping = tokenized_inputs[\"overflow_to_sample_mapping\"]\n",
    "\n",
    "    for i in range(len(tokenized_inputs[\"input_ids\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        original_labels = examples[\"ner_tags\"][sample_index]\n",
    "\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(original_labels):\n",
    "                    label_ids.append(original_labels[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs.pop(\"offset_mapping\", None)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict(tokenize_and_align_labels(ner_dataset['train'].to_dict())),\n",
    "    'validation': Dataset.from_dict(tokenize_and_align_labels(ner_dataset['validation'].to_dict()))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d705079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 41:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.245284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.171927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.159005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.163453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.162199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=310, training_loss=0.28681788290700605, metrics={'train_runtime': 2514.0527, 'train_samples_per_second': 1.965, 'train_steps_per_second': 0.123, 'total_flos': 405707188378104.0, 'train_loss': 0.28681788290700605, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./model/ner-indobert-p2\",\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d6f0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./model/ner-indobert-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a08e87",
   "metadata": {},
   "source": [
    "### Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfd551b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [ID2LABEL[label] for label in label_row if label != -100]\n",
    "        for label_row in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [ID2LABEL[pred] for pred, label in zip(pred_row, label_row) if label != -100]\n",
    "        for pred_row, label_row in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"report\": classification_report(true_labels, true_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595cd93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.compute_metrics = compute_metrics\n",
    "eval_model = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0875432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss\n",
      "0.1590048372745514\n",
      "\n",
      "\n",
      "eval_precision\n",
      "0.7896825396825397\n",
      "\n",
      "\n",
      "eval_recall\n",
      "0.805668016194332\n",
      "\n",
      "\n",
      "eval_accuracy\n",
      "0.9540736080528468\n",
      "\n",
      "\n",
      "eval_f1\n",
      "0.7975951903807615\n",
      "\n",
      "\n",
      "eval_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ACTION       0.95      0.93      0.94       151\n",
      "      KONTAK       0.50      0.80      0.62        10\n",
      "     LAYANAN       0.71      0.57      0.63        21\n",
      "       MODUS       0.86      0.86      0.86       165\n",
      "     NOMINAL       0.81      0.85      0.83        26\n",
      "      PERSON       0.63      0.53      0.58        32\n",
      "    PLATFORM       0.65      0.81      0.72        27\n",
      "      PRODUK       0.49      0.57      0.53        54\n",
      "         REK       0.57      0.50      0.53         8\n",
      "\n",
      "   micro avg       0.79      0.81      0.80       494\n",
      "   macro avg       0.69      0.71      0.69       494\n",
      "weighted avg       0.80      0.81      0.80       494\n",
      "\n",
      "\n",
      "\n",
      "eval_runtime\n",
      "4.7887\n",
      "\n",
      "\n",
      "eval_samples_per_second\n",
      "22.135\n",
      "\n",
      "\n",
      "eval_steps_per_second\n",
      "1.462\n",
      "\n",
      "\n",
      "epoch\n",
      "5.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in eval_model.keys():\n",
    "    print(k)\n",
    "    print(eval_model[k])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16432f2",
   "metadata": {},
   "source": [
    "# Model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "629f5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/ner-indobert-finetuned\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5edf52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3005 entries, 0 to 3004\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   waktu       3005 non-null   object\n",
      " 1   clean_text  3005 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 47.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_teks = pd.read_csv('data/clean_datatext.csv')\n",
    "df_teks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ae15a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3005 entries, 0 to 3004\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   text_ID     3005 non-null   int64 \n",
      " 1   waktu       3005 non-null   object\n",
      " 2   clean_text  3005 non-null   object\n",
      " 3   len_text    3005 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 94.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_teks.reset_index(inplace=True)\n",
    "df_teks.rename(columns={\n",
    "    'index':'text_ID'\n",
    "}, inplace=True)\n",
    "df_teks['len_text'] = df_teks['clean_text'].str.len()\n",
    "df_teks.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fdd87e",
   "metadata": {},
   "source": [
    "## Building pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251dbf56",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c82b0fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sebelumnya mohon maaf kalau ada kata-kata saya yg kasar, mengumpat dan gak enak dibaca) temen-temen yg merasa dirugikan materi, boleh dm saya untuk mencatat nominal kerugiannya. sudah terdata 42 orang korban. saya nyesel enggak speak up dan cari tau tentanguser-11350117449701790883dari dulu. ada urusan piutang sejak bulan agustus sebesar 925rb. tapi karena saya ga enakan dan memaklumi segala musibah dia, pokoknyauser-11350117449701790883ini jago banget gaslighting, seolah-olah bikin saya merasa bersalah dan ga sabaran karena terus-terusan nagih uangnya. barusan saya udah investigasi. anjay investigasi bikin saya lemes bgt ga nyangkauser-11350117449701790883bener-benerseorang penipu.semua keluarga yg ada di ceritanya dia alhamdulillahsemuanya masih hidup.ada beberapa bukti yg saya foto dan saya ngobrol sama keluarga tetangganya. untuk temen-temen yg udah tertipu sama dia, jangan harap uangnya balik lagi. ikhlasin aja dan doain rame-rame semoga kagiles treuk, gak deng, doain aja. terserah. saya kesel. dan tolong banget untuk laporin rekening mandiri dia 9000029242642 a/nsuhermansyahke atmnya dibekuin, supaya enggak ada lagi yg tertipu untuk ngasih bantuan/ ngasih pinjam uang/ beli bukunyauser-11350117449701790883(udah tutup akun si bgst ini) astagfirullah. temen-temen yg beli buku dia yg saya tau ada 100 orang lebih. belum termasuk modus tipu daya yg lain. guguk benar ni spesies. haduh, habis sudah kesabaran saya. nanti dilanjut ya. saya baru sampai rumah dan tiba-tiba ga enak badan. nanti saya ceritain kronologi dan cerita lengkapnya. makasih banyak terutama untuk kangroy irawanyg udah bantu saya nyari segala informasi tentang penipu ini. cerita kali ini akan sangat panjaaang. saya mau beberin dulu kronologi saya kenapa bisa sampe ditipu samauser-11350117449701790883. berawal dari kemunculan dia di quora yg tentang patah hati terberat dan diselingkuhi, karena sesama orang bandung saya menawarkan diri untuk sekedar ngopi-ngopi syantik. aduh murahan banget saya kalau dipikir-pikir. dan direspon donggg. chattan sama dia mengalir aja, seru, asik, adaaa aja yg diobrolin, orangnya humoris, dan juga bijak. karena chatting di dm quora kurang nyaman, saya minta nomor wa/telegramnya tapi ga punya hp katanya, jadilah kita chatting via facebook. dia ga pake akun sendiri karena udah nyaman seperti itu yg ga dikenal sama orang-orang.jadi, dia pake akun jual beliyg belum di-log out di pc warnet. terus dia baca tulisan saya yg lagi di fase terendah, betapa saya banyak kesulitan karena memiliki gangguan pendengarandan gak mampu beli alat bantu dengar. dia bilang,\"masalah saya cemen banget cuma patah hati tapi mengurung diri sampe 2 tahun dibandingkan dengan masalah kamu.\" ceritanya dia punya harta tidak bergerak, deposito yg kalau dicairin jumlahnya 24 25jt, nyari uang tambahannya dengan bisnis kaos untuk komunitas. kebetulan langsung ada 3 orderan ke dia, yg ngehubungin via email! aneh banget kalau dipikir lagi. gemeter banget, berasa dapet keajaiban dari allah. berasa doa-doa saya terjawab saat itu wkwkwk. ini tuh saya harus ambil keputusan saat itu juga. ya mana saya percaya dong. tapi emang pandai bersilat lidah, bisaaa aja bikin orang percaya. ah atau memang sayanya yg gampang dibegoin wkwkwk gilak ni orang baik banget. anak sultan yg lagi merendah kali ya (pikiran naif saya kala itu) cakep banget kaaan sanjungannya. huekk bego banget kenapa dulu percaya! padahal feeling udah ngeganjel banget tentang dia tapi saya berusaha untuk mikir positif. dia ngedoain dirinya sendiri guys!user-11350117449701790883yg nipu,user-11350117449701790883juga yg hidupnya nanti ga akan beruntung ini ceritanya saya ngambek sama dia, wifinya error, udah berhari-hari ngewarnet dan makan di luar, boros uang. saya punya temen di telkom yg bisa dimintain tolong buat mengerahkan teknisi untuk dateng ke rumah dia. tapi dia ga ngasih data inetnya. padahal disitu saya pengen tau banget alamat dia dimana. terlalu privacy kali ya wkwk. bisnis dimulai dia memanfaatkan kelemahan saya soal hitungan dan gak fokus. kalau tentang angka-angka gitu otak saya seperti menolak, dari sd. wkwkwk. jadi ya saya mah percaya-percaya aja sama dia. ini saya kena rayuan dia, yg semakin cepat dana terkumpul, semakin cepat orderan diproses, akan semakin besar juga laba yg kita dapet. waktu itu hari minggu, gak ada toko emas yg buka selain di pasar. cincin 1,4gram saya cuma keterima 700rb nyesek banget. tapi gapapa, demi bisa beli abd (alat bantu dengar) masalah mulai mengeruh. setelah saya mastiin bahwa si suher ini bukan orang nipu, karena sebelumnya udah survey tempat sablonnya. eh duitnya ketelen, kartu atmnya keluar tapi ga ada saldo masuk. nungguin teknisi dateng dan abis pulsa sampe 50rb buat nelponin cs bank mandiri. tapi estimasi pengembalian uangnya ga jelas. katanya 2x24 jam. ditunggu seminggu, dua minggu ga cair-cair katanya. sampe 2 bulan lamanya. adaaa aja alesan si suher ini, sampe capek sendiri sama dia. eh ini udah ada tanda-tanda psikopat gak sih dia? mulai ngegaslighting. bikin saya merasa bersalah. cerdas nianuser-11350117449701790883ini, manipulatifnya bisa bangetttt. nyahahahaha dan berbagai alasan-alasan lain yg gak saya sebutin di sini saya capek nanggepinnya. lalu dia tiba-tiba dihujani berbagi musibah (lah anyink saya juga orang susah! kapan duit saya balik, tapi saya masih tetep sabar, maklumi dia, kasihani dia, eh dianya ga kasihan sama saya. bangst!) minta kontak dania ga dikasih, katanya biar itu jadi urusan dia aja. beberapa hari kemudian semua orderan bermasalah :) bullshiiiit kebanyakan janji! wow drama banget. cocok nih buat script ftv indosiar dulu mah kasihan, ga tega bacanya. sekarang semua tentang dia saya benci dan jijik! terus katanya uangnya awal bulan oktober udah pasti cair, nomor rekening saya udah didaftarin. oh ya saya seneng donggg, akhirnya uang saya kembali. tapi saya bener-bener overthinking banget, takut dia mati beneran. taunya sampe sekarang masih hidup dan lancar jaya nipu orang-orang. padahal saya udah orderin dia grabfood, karena berminggu-minggu ga pulang, diusir, dan gak punya uang. cuma sekali sih ngordernya. saya mah disini makan telor ceplok juga cukup, padahal waktu itu sisa ovo saya pengen dibeliin martabak:') terus mau ngeteng ke banjar, pasca bapak tirinya kecelakaan. saya sampe dm kang yosep, semoga kang yosep ada di bandung tadinya mau minta tolong angkut itu beban masyarakat buat ke rs di banjar. dikasih uang bekal sama kangroy irawan, pagi-pagi jam 6 saya mandi, dingin banget tulung, rhinitis saya kambuh lagi. nyusulinuser-11350117449701790883ke warnet untuk nguatin dia atas segala cobaan yg menimpa, kan ceritanya mau jadi anak sebatang kara. heeh da didinya teh hachi lain, tapi hak.cuih! (astagfirullah mamprang banget dede) saya udah hilang respect sejak cerita yg abahnya meninggal dan mamanya koma, terus kalimat penguat dari seorang hanifah yg saya pikir\"masa anak umur segitu pikirannya bijaksana banget, sebrilliant itu, wow.\"tapi saya mencoba untuk tetap percaya dan mendoakan semoga segala masalahnya cepet kelar. takut banget suudzon sama yg lagi kena musibah entar malah saya yg dosa. eh ternyata biang dosanya akang suher sendiri semua jalan terbuka pas liat postinganuser-11350117449701790883yg galang dana untuk tahlilan hanifah, dikomen sama mbakolvin, tapi postingannya keburu dihapus dan saya ga sempat capture. kangroy irawanjuga banyak membantu saya ngasih gambaran arah peta untuk saya yg buta maps ini. wkwkwkw. alhamdulillah hari tadi semuanya lancar. dimudahkan sama allah. meski panas terik dan lambung kambuh telat makan dan terapi jantung karena fakta-fakta yg terungkap di lapangan tadi. hebat sih udah masuk golongan profesional nipunya. gile sambil kalian baca kronologi saya yg panjang dan berbelit ini, selanjutnya saya mau ceritain investigasi hari ini sama sepupu saya. maaf ya bertele-tele hehe. rumah saya di daerah gunung, tapi tadi terasa deket banget ke arcamanik. tanpa liat maps, saya dimudahkan pergi ke tempat sablon yg dulu saya survey. saya gali informasi dari mamang sablon, bahwauser-11350117449701790883ga ngorder apa-apa dan belum menyerahkan uang sepeserpun. mamang sablon geleng-geleng kepala pas saya bilang abangnya a denis meninggal. bahwasannya mereka teman dekat dan selalu diwanti-wanti supaya jangan ngambil orderan dari si denis katanya. saya dimintain nomer hp dan dikasih tau alamat rumahnya. keluar dari komplek mamang sablon, nanya-nanyalah ke orang yg lagi nangkring di sana. saya samperin laki-laki yg lagi jaga depot isi ulang. \"a maaf mau tanya, tau orang ini a? rumahnya sebelah mana?\"(saya sodorin foto akang penipu) \"oh iya tau. dari sini belok kanan dikit yg ada warung.\" \"aa suka liat a denis gak? kalo abangnya ada?\" \"ada, tiap hari juga ketemu sama abangnya. tapi kalau denis mah saya jarang ketemu\"(nah buset dah tuh syok banget kan, abangnya masih hidup!) \"kalo a jodi, tau gak rumahnya dimana?\"(jodi itu informan, yg suka bantuin dan anter denis kemana-mana) \"jodi mah masih keluarga sama saya, ada, rumahnya di belakang sini.\" saya mengurungkan niat untuk ketemu jodi. saya mau nyari rumahnyauser-11350117449701790883dulu. saya tau itu rumah a denis karena ada ibu-ibu yg fotonya pernah diposting di tulisan dia. iya. itu mamanya!iya, mamanya sehat walafiat. mamanya itu awalnya mengelak gatau yg namanya denis. ah mamanya tidak pintar bersandiwara saya langsung nunjuk ke belakangnya beliau, di belakang etalase warung ada ruang tamu, ada abangnya dan anak-anak. abangnya mirip banget samauser-11350117449701790883. ngobrolah kami disana. tada semuanya masih hidup dan sehat. informasi dari abangnya,user-11350117449701790883udah lama ga pulang. berbulan-bulan. terakhir posisinya lagi di kuningan. dan dia baru tau kalau dh ini penulis. keluarga udah lama menanggung malu dan habis segalanya karena ulah si penipu ini! jadi ya mereka ga aneh dapet kabar kayak gini, mamanya malah ogah-ogahan nanggepin. keluarganya udah angkat tangan juga. abangnya bilang, sejak lulus sma selalu nipu orang, temennya, sodaranya, orang di kontrakan, bahkan abangnya sendiri juga ditipu. jadi terserah aja katanya mau ditangkep ya silakan, mau dicari juga bingung nyari kemana. padahal dh ini anaknya emang pinter. pernah populer di komunitas apaaa gitu lupa, kerjasama bareng bpk. ridwan kamil. sangat disayangkan prestasi dan kepintaran buat nipu orang. oh ya, dh nipu karena suka judi katanya. hmmm judi soal istri abangnya yg depresi,itu bohong.mama hanifah ga lagi hamil dangak melahirkan anak yg namanya hanafi.malah istrinya lagi kerja siang tadi. soal abah tirinya yg katanya meninggal,itu juga bohong. beliau lagi sakit lambung dan diem di kamar, dikasih liat obat-obatan dari dokter. soalhanifahyg meninggal dari selokan,itu juga bohong parah.nih, saya tunjukkin selfie saya sama hanifah dek hanifah, mamang kamu jahat! jual nama kamu buat mengemis iba doang. itu memang gambar yg dibikin sama hanifah pas hari ayah. diposting sama abangnya, mungkin langsung dicomot sama dh dengan segala karangan ceritanya. soal dia yg punya sejumlah uang dan bangun kontrakkan untuk mamanya,itu halu.ada sih di belakang kontrakkan, tapipunya orang lain,bukan punyanya si pedut. soal modem di rumahnya yg ngadat tapi nolak dimintain nomor inet,itu juga bohong.ga ada komputer di rumah, makanya ke warnet terus. kampret betul. soal keributan harta tanah mamanya yg dijual sama saudara tirinya juga bohong. yg ada keluarganya jual sawah buat lunasi hutang si anak gatau diri itu! soal dh yg katanya karantina positif corona,itu bohong banget.orang dia ga pulang-pulang ke rumah. apa mau kena kualat terpapar corona beneran kali ya! hadeuhhh emosi saya alhamdulillah disuguhi frestea dingin, walau hati dan kepala panas, tenggorokan harus adem ye. selanjutnya tujuan saya ke warnet yg ada di seberang rs. al-islam. saya nyari di bilik sebelah kiri. sepupu saya nyari di bilik sebelah kanan. seru. kayak menjalankan sebuah misi. padahal di warnetnya lagi ga ada siapa-siapa selain operator warnet hahahaha ternyata, denis herman udah ga pernah lagi ke warnet sejak 10 hari yg lalu. saya minta tolong op warnet untuk ngabarin saya kalau-kalau dh balik lagi ke warnet. (aduh, saya takut si dh ini baca pake fake account dan kongkalikong sama op warnetnya. haaaa saya saya gak suka berburuk sangka gini teh, ga enak hati) saya bergegas ke bank mandiri, udah dapet nomer antrian, tetep aja harus lapor polsek. bikin laporannya harus di polres dekat tkp. saya lagi di bank mandiri soetta tadi, harus nyari polres di daerah ujung berung karena saya transaksinya di dekat mutiara kitchen waktu itu. soetta-ujung berung, saya udah ga ada lagi tenaga untuk lanjutin investigasi. makanya, sekali lagi saya minta temen-temen untuk laporin rekening nomer rekening dh, ya. udah saya cantumin di awal tadi. please. oh ya, ini barangkali mau lihat lagi wajah kangtip dhuarrrr sekarang, saya bingung, malu, takut, gimana bilang sama mama soal saya udah ditipu ini? huhu, pasti mama kecewa dan marah banget. itu uang 925rb separuhnya punya mama. mama tuh bukan tipe yg friendly, tapi lebih ke\"rasain noh ditipu! jangan gampang percaya sama orang, blablabla\"dan segala perkataan yg mungkin akan terdengar ga enak untuk saya minta doanya temen-temen, semoga saya bisa cepet dapet kerjaan lagi. supaya saya punya penghasilan lagi dan gantiin uang mama lebih dari ini dan kalaupun mama tau soal kasus penipuan ini, semoga hatinya lekas legowo ps: hi, denis herman, yg barangkali lagi baca tulisan ini pake fake account. saya mungkin emang gampang dibodohi dan diperdaya sama kamu. tapi kebenaran sedang allah tunjukkan sekarang. kamu sendiri bilang, kalau nipu, hidupnya ga akan beruntung. selamat menghadapi ketidakberuntungan, ya ninuninuninu ohiya, saya baru inget 1 lagi. kangtip ini suka bilang:\"saya orangnya pelupa banget. jadi saya ga bisa berbohong. soalnya harus nginget-nginget apa yg udah diceritain.\"saya ga sreg banget hati pas dia berkali-kali bilang kayak gitu, oalaaaah itu cuma untuk menutupi kebobrokan dia sendiri yg tukang bohong eh iyaa saya inget juga, si dh ini bilang suka minjemin duit ke tetangga-tetangganya. saya dikasih liat list hutang mereka nominalnya jutaan sampe 25juta! gileee saya pikir si dh ini emang beneran orang kaya, tapi pas ditagih ke mereka barang 100ribupun ga ada yg bayar. yg saya curigain dari list itu, tulisannya rata, sama, aduh susah jelasinnya, kayak baru ditulis kemarin sore. hellooow gaya bener ngasih pinjam duit sampe 25jt padahal mah cuma ingin membangun imagenya agar terlihat seperti orang kaya baik hati dan gak berdaya karena minjemin duit ke penghutang yg susah bayar semoga temen-temen yg udah tertipu secara materi, diganti rezekinya berkali lipat dan jadi ladang pahala untuk kalian. jgn berhenti berbuat baik, tapi di lain waktu harus lebih hati-hati.\n"
     ]
    }
   ],
   "source": [
    "long_idx = df_teks['len_text'].argmax()\n",
    "longest_text = df_teks.iloc[long_idx, 2]\n",
    "print(longest_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d71b60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    longest_text,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    max_length=250,\n",
    "    stride=128,\n",
    "    return_tensors='pt',\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "dict(tokenized_inputs).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66ca588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = model(\n",
    "    input_ids = tokenized_inputs['input_ids'],\n",
    "    token_type_ids = tokenized_inputs['token_type_ids'],\n",
    "    attention_mask = tokenized_inputs['attention_mask'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5505924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 250])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ff3cda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 250, 19])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_model['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "766b02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logits = pred_model['logits']\n",
    "pred_ID = pred_logits.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb7c5445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACTION': ['mengu',\n",
       "  'dirugikan',\n",
       "  'dm',\n",
       "  'mencatat',\n",
       "  'terd',\n",
       "  'spe',\n",
       "  'cari',\n",
       "  'nag',\n",
       "  '##ih',\n",
       "  'investigasi',\n",
       "  'balik',\n",
       "  'tr',\n",
       "  'lapor'],\n",
       " 'KONTAK': ['113',\n",
       "  '##117',\n",
       "  '##44',\n",
       "  '##97',\n",
       "  '##01',\n",
       "  '##79',\n",
       "  '##08',\n",
       "  '113',\n",
       "  '##50',\n",
       "  '##117',\n",
       "  '##44',\n",
       "  '##97',\n",
       "  '##01',\n",
       "  '##79',\n",
       "  '##08',\n",
       "  '-',\n",
       "  '113',\n",
       "  '##50',\n",
       "  '##117',\n",
       "  '##44',\n",
       "  '##97',\n",
       "  '##01',\n",
       "  '##79',\n",
       "  '##08',\n",
       "  '##02'],\n",
       " 'LAYANAN': [],\n",
       " 'MODUS': ['piutang', 'gas', '##light', '##ing', 'penipu', 'tertipu'],\n",
       " 'NOMINAL': ['nominal',\n",
       "  'kerugian',\n",
       "  '92',\n",
       "  '##5',\n",
       "  '##rb',\n",
       "  'uangnya',\n",
       "  'uangnya',\n",
       "  '9000'],\n",
       " 'PERSON': ['##user', '##83', 'pokoknya', '##user', '##83'],\n",
       " 'PLATFORM': [],\n",
       " 'PRODUK': [],\n",
       " 'REK': ['##50', '##83', 'rekening', 'mandiri']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inp = 0\n",
    "sentence_exm = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][batch_inp])\n",
    "idlabel_exm = pred_ID[batch_inp]\n",
    "word_id = tokenized_inputs.word_ids(batch_inp)\n",
    "word_mapping = tokenized_inputs['offset_mapping'][batch_inp]\n",
    "\n",
    "\n",
    "dct_temp = {\n",
    "        'ACTION':[],\n",
    "        'KONTAK':[],\n",
    "        'LAYANAN':[],\n",
    "        'MODUS':[],\n",
    "        'NOMINAL':[],\n",
    "        'PERSON':[],\n",
    "        'PLATFORM':[],\n",
    "        'PRODUK':[],\n",
    "        'REK':[]\n",
    "    }\n",
    "\n",
    "for w, idlbl, wid, wmap in zip(sentence_exm, idlabel_exm, word_id, word_mapping):\n",
    "    if idlbl != 18:\n",
    "        lbl = ID2LABEL[int(idlbl)]\n",
    "        dct_temp[lbl[2:]].append(w)\n",
    "\n",
    "dct_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b217ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_token(list_token : list) -> tuple:\n",
    "    full_word = ''\n",
    "    label = 'O'\n",
    "    for i in range(len(list_token)):\n",
    "        token = list_token[i][0]\n",
    "        token = token.replace('##', '')\n",
    "        full_word = full_word + token\n",
    "        if (list_token[i][1] != 'O') and (label == 'O'):\n",
    "            label = list_token[i][1]\n",
    "    \n",
    "    return (full_word, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f6c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACTION': ['mengumpat',\n",
       "  'dirugikan',\n",
       "  'dm',\n",
       "  'mencatat',\n",
       "  'terdata',\n",
       "  'speak',\n",
       "  'cari',\n",
       "  'nagih',\n",
       "  'investigasi',\n",
       "  'balik',\n",
       "  'treuk',\n",
       "  'laporin'],\n",
       " 'KONTAK': ['11350117449701790883dari',\n",
       "  '11350117449701790883ini',\n",
       "  '-',\n",
       "  '11350117449701790883bener'],\n",
       " 'LAYANAN': [],\n",
       " 'MODUS': ['piutang', 'gaslighting', 'penipu', 'tertipu'],\n",
       " 'NOMINAL': ['nominal', 'kerugiannya', '925rb', 'uangnya', 'uangnya'],\n",
       " 'PERSON': ['tentanguser', 'pokoknyauser'],\n",
       " 'PLATFORM': [],\n",
       " 'PRODUK': [],\n",
       " 'REK': ['rekening', 'mandiri']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_temp = {\n",
    "        'ACTION':[],\n",
    "        'KONTAK':[],\n",
    "        'LAYANAN':[],\n",
    "        'MODUS':[],\n",
    "        'NOMINAL':[],\n",
    "        'PERSON':[],\n",
    "        'PLATFORM':[],\n",
    "        'PRODUK':[],\n",
    "        'REK':[]\n",
    "    }\n",
    "\n",
    "lastid_batch = -1\n",
    "past_wid = None\n",
    "list_to_merge = []\n",
    "is_added = False\n",
    "past_added_lbl = None\n",
    "for w, idlbl, wid, wmap in zip(sentence_exm, idlabel_exm, word_id, word_mapping):\n",
    "    if wid == None:\n",
    "        continue\n",
    "    elif wid < lastid_batch:\n",
    "        continue\n",
    "    else:\n",
    "        lastid_batch = wid\n",
    "\n",
    "    lbl = ID2LABEL[int(idlbl)]\n",
    "    if wid == past_wid:\n",
    "        list_to_merge.append((w, lbl))\n",
    "        if is_added:\n",
    "            dct_temp[past_added_lbl].pop()\n",
    "            is_added = False\n",
    "        continue\n",
    "    else:\n",
    "        if len(list_to_merge) > 1:\n",
    "            wpast, lblpast = merge_token(list_to_merge)\n",
    "            if lblpast != 'O':\n",
    "                dct_temp[lblpast[2:]].append(wpast)\n",
    "        past_wid = wid\n",
    "        list_to_merge = [(w, lbl)]\n",
    "\n",
    "\n",
    "    if lbl != 'O':\n",
    "        is_added = True\n",
    "        dct_temp[lbl[2:]].append(w)\n",
    "        past_added_lbl = lbl[2:]\n",
    "    else:\n",
    "        is_added = False\n",
    "\n",
    "dct_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "464415fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_temp = {\n",
    "        'ACTION':[],\n",
    "        'KONTAK':[],\n",
    "        'LAYANAN':[],\n",
    "        'MODUS':[],\n",
    "        'NOMINAL':[],\n",
    "        'PERSON':[],\n",
    "        'PLATFORM':[],\n",
    "        'PRODUK':[],\n",
    "        'REK':[]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3ae5b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractionbatch_result(dct_container, words_list, idlabel, word_id, word_mapping, lastid_batch):\n",
    "    past_wid = None\n",
    "    list_to_merge = []\n",
    "    is_added = False\n",
    "    past_added_lbl = None\n",
    "    for w, idlbl, wid, wmap in zip(words_list, idlabel, word_id, word_mapping):\n",
    "        if wid == None:\n",
    "            continue\n",
    "        elif wid < lastid_batch:\n",
    "            continue\n",
    "        else:\n",
    "            lastid_batch = wid - 1\n",
    "\n",
    "        lbl = ID2LABEL[int(idlbl)]\n",
    "        if wid == past_wid:\n",
    "            list_to_merge.append((w, lbl))\n",
    "            if is_added:\n",
    "                dct_container[past_added_lbl].pop()\n",
    "                is_added = False\n",
    "            continue\n",
    "        else:\n",
    "            if len(list_to_merge) > 1:\n",
    "                wpast, lblpast = merge_token(list_to_merge)\n",
    "                if lblpast != 'O':\n",
    "                    dct_container[lblpast[2:]].append(wpast)\n",
    "            past_wid = wid\n",
    "            list_to_merge = [(w, lbl)]\n",
    "\n",
    "\n",
    "        if lbl != 'O':\n",
    "            is_added = True\n",
    "            dct_container[lbl[2:]].append(w)\n",
    "            past_added_lbl = lbl[2:]\n",
    "        else:\n",
    "            is_added = False\n",
    "    \n",
    "    return lastid_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e4b44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prediction(pred_model, tokenized_inputs):\n",
    "    dct_temp = {\n",
    "            'ACTION':[],\n",
    "            'KONTAK':[],\n",
    "            'LAYANAN':[],\n",
    "            'MODUS':[],\n",
    "            'NOMINAL':[],\n",
    "            'PERSON':[],\n",
    "            'PLATFORM':[],\n",
    "            'PRODUK':[],\n",
    "            'REK':[]\n",
    "        }\n",
    "\n",
    "    pred_logits = pred_model['logits']\n",
    "    pred_ID = pred_logits.argmax(dim=2)\n",
    "\n",
    "    lastid_batch = -1\n",
    "    for i in range(pred_ID.size()[0]):\n",
    "        batch_inp = 0\n",
    "        wordslist = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][batch_inp])\n",
    "        idlabel = pred_ID[batch_inp]\n",
    "        word_id = tokenized_inputs.word_ids(batch_inp)\n",
    "        word_mapping = tokenized_inputs['offset_mapping'][batch_inp]\n",
    "\n",
    "        lastid_batch = extractionbatch_result(\n",
    "            dct_container= dct_temp,\n",
    "            words_list=wordslist,\n",
    "            idlabel= idlabel,\n",
    "            word_id= word_id,\n",
    "            word_mapping= word_mapping,\n",
    "            lastid_batch= lastid_batch\n",
    "        )\n",
    "\n",
    "    return dct_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91826617",
   "metadata": {},
   "source": [
    "## Full NER extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6bb345e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3005/3005 [06:32<00:00,  7.66it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ID</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>KONTAK</th>\n",
       "      <th>LAYANAN</th>\n",
       "      <th>MODUS</th>\n",
       "      <th>NOMINAL</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>PLATFORM</th>\n",
       "      <th>PRODUK</th>\n",
       "      <th>REK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>nomer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>jual | beli</td>\n",
       "      <td>scam | skema</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mobil</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>love | scamming | internasional</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>3000</td>\n",
       "      <td>ketipu</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>scam</td>\n",
       "      <td></td>\n",
       "      <td>cindy | cindy | cindy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>3001</td>\n",
       "      <td>beli | wts | ketipu | beli</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>scam</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>cat | cat | 2 | tiketnya | cat | 2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>3002</td>\n",
       "      <td>ketipu</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>scam</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>x</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>3003</td>\n",
       "      <td>beli | ketipu</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>penipuan | scam</td>\n",
       "      <td>jutaan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>baju</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>3004</td>\n",
       "      <td>beli | ketipu</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>scam</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>tds3 | section</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3005 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_ID                      ACTION KONTAK      LAYANAN  \\\n",
       "0           0                              nomer                \n",
       "1           1                                     jual | beli   \n",
       "2           2                                                   \n",
       "3           3                                                   \n",
       "4           4                                                   \n",
       "...       ...                         ...    ...          ...   \n",
       "3000     3000                      ketipu                       \n",
       "3001     3001  beli | wts | ketipu | beli                       \n",
       "3002     3002                      ketipu                       \n",
       "3003     3003               beli | ketipu                       \n",
       "3004     3004               beli | ketipu                       \n",
       "\n",
       "                                MODUS NOMINAL                 PERSON PLATFORM  \\\n",
       "0                                                                               \n",
       "1                        scam | skema                                           \n",
       "2                                                                               \n",
       "3                                                                               \n",
       "4     love | scamming | internasional                                           \n",
       "...                               ...     ...                    ...      ...   \n",
       "3000                             scam          cindy | cindy | cindy            \n",
       "3001                             scam                                           \n",
       "3002                             scam                                       x   \n",
       "3003                  penipuan | scam  jutaan                                   \n",
       "3004                             scam                                           \n",
       "\n",
       "                                  PRODUK REK  \n",
       "0                                             \n",
       "1                                  mobil      \n",
       "2                                             \n",
       "3                                             \n",
       "4                                             \n",
       "...                                  ...  ..  \n",
       "3000                                          \n",
       "3001  cat | cat | 2 | tiketnya | cat | 2      \n",
       "3002                                          \n",
       "3003                                baju      \n",
       "3004                      tds3 | section      \n",
       "\n",
       "[3005 rows x 10 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_extraction = {\n",
    "    'text_ID':[],\n",
    "    'ACTION':[],\n",
    "    'KONTAK':[],\n",
    "    'LAYANAN':[],\n",
    "    'MODUS':[],\n",
    "    'NOMINAL':[],\n",
    "    'PERSON':[],\n",
    "    'PLATFORM':[],\n",
    "    'PRODUK':[],\n",
    "    'REK':[]\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(df_teks))):\n",
    "    dct_extraction['text_ID'].append(df_teks.iloc[i, 0])\n",
    "    text = df_teks.iloc[i, 2]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=250,\n",
    "        stride=128,\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )\n",
    "    pred_model = model(\n",
    "        input_ids = tokenized_inputs['input_ids'],\n",
    "        token_type_ids = tokenized_inputs['token_type_ids'],\n",
    "        attention_mask = tokenized_inputs['attention_mask'],\n",
    "    )\n",
    "    result_extraction = extract_prediction(pred_model, tokenized_inputs)\n",
    "\n",
    "    for key, val in result_extraction.items():\n",
    "        dct_extraction[key].append(' | '.join(val))\n",
    "\n",
    "df_extraction = pd.DataFrame.from_dict(dct_extraction)\n",
    "df_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "584dbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extraction.to_csv('data/extraction_result.tsv', sep='\\t', index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GMT25_venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
